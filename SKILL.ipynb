{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1YpfcqQRyrEFYaJUPpn1-57-HYKe0QgEC",
      "authorship_tag": "ABX9TyMIhjBKzyQBtQPTYFAais89",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshthakare16/skill2/blob/main/SKILL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone \"https://github.com/anshthakare16/skill2.git\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WK_MlfqQ_IB7",
        "outputId": "d716c067-75a1-4c6a-d8ed-3915c6177053"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'skill2' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SKILL EXPERIMENT 1-2**"
      ],
      "metadata": {
        "id": "z-LnIZa34-2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The following the preprosessing steps and methods to upload and extract the database.**"
      ],
      "metadata": {
        "id": "aWoKwbcwR5VA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WiNP0W2iNX2"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df= pd.read_csv(r\"/content/drive/My Drive/hi.csv\")\n",
        "df.head(200)\n",
        "\n",
        "\n",
        "df.dropna(axis=0,inplace=True)\n",
        "print(df)\n",
        "\n",
        "print(\"info\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"describe\")\n",
        "print(df.describe())\n",
        "\n",
        "print(\"IS NULL any\")\n",
        "print(df.isnull().any())\n",
        "print(\"isnullsum\")\n",
        "print(df.isnull().sum())\n",
        "print(\"duplicated\")\n",
        "print(df.duplicated())\n",
        "print(\"drop duplicates\")\n",
        "print(df.drop_duplicates(inplace=True))\n",
        "print(\"DROPNA\")\n",
        "print(df.dropna(inplace=True))\n",
        "output_path = '/content/drive/My Drive/hi2.csv'\n",
        "df.to_csv(output_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lable and one hot encoding or one-hot encoding. EXPERIMENT **3**"
      ],
      "metadata": {
        "id": "uK5nhBGD_VpL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply encoding techniques such as One-Hot Encoding, Label Encoding, or Ordinal Encoding to convert categorical features into numerical values. This makes the data suitable for machine learning models."
      ],
      "metadata": {
        "id": "J-zn1S8STWAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "# Copy original dataset for encoding\n",
        "df_encoded = df.copy()\n",
        "\n",
        "# Label Encoding for categorical columns\n",
        "label_encoders = {}\n",
        "for col in ['Gender', 'Physical_Activity_Level', 'Stress_Level', 'Chest_Pain_Type',\n",
        "            'Thalassemia', 'ECG_Results', 'Heart_Attack_Risk']:\n",
        "    le = LabelEncoder()\n",
        "    df_encoded[col + '_Label'] = le.fit_transform(df_encoded[col])\n",
        "    label_encoders[col] = le  # Store the encoder for reference\n",
        "\n",
        "# One-Hot Encoding for categorical columns\n",
        "one_hot_encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
        "one_hot_encoded = one_hot_encoder.fit_transform(df[['Gender', 'Physical_Activity_Level',\n",
        "                                                    'Stress_Level', 'Chest_Pain_Type',\n",
        "                                                    'Thalassemia', 'ECG_Results', 'Heart_Attack_Risk']])\n",
        "\n",
        "# Convert One-Hot Encoded array to DataFrame\n",
        "one_hot_df = pd.DataFrame(one_hot_encoded, columns=one_hot_encoder.get_feature_names_out())\n",
        "\n",
        "# Concatenate One-Hot Encoded columns with original dataframe\n",
        "df_encoded = pd.concat([df_encoded, one_hot_df], axis=1)\n",
        "\n",
        "# Drop original categorical columns\n",
        "df_encoded.drop(columns=['Gender', 'Physical_Activity_Level', 'Stress_Level',\n",
        "                         'Chest_Pain_Type', 'Thalassemia', 'ECG_Results', 'Heart_Attack_Risk'], inplace=True)\n",
        "\n",
        "# Display the first few rows of the encoded dataset\n",
        "df_encoded.head()"
      ],
      "metadata": {
        "id": "198Epf-m-FUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SKILL EXPERIMENT-4**"
      ],
      "metadata": {
        "id": "WazJ_LOIF-ub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize and scale numerical data using Min-Max Scaling or Standard Scaling to bring all features to a similar scale. Also, balance the dataset using techniques like SMOTE or undersampling to handle class imbalance."
      ],
      "metadata": {
        "id": "CVEsVT31Tjvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler, StandardScaler\n",
        "\n",
        "# Label Encoding for categorical columns\n",
        "categorical_cols = ['Gender', 'Physical_Activity_Level', 'Stress_Level', 'Chest_Pain_Type',\n",
        "                    'Thalassemia', 'ECG_Results', 'Heart_Attack_Risk']\n",
        "\n",
        "df_encoded = df.copy()\n",
        "label_encoders = {}\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if col in df_encoded.columns:  # Ensure the column exists\n",
        "        le = LabelEncoder()\n",
        "        df_encoded[col + '_Label'] = le.fit_transform(df_encoded[col])\n",
        "        label_encoders[col] = le  # Store the encoder for reference\n",
        "\n",
        "# One-Hot Encoding\n",
        "one_hot_encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
        "one_hot_encoded = one_hot_encoder.fit_transform(df_encoded[categorical_cols])\n",
        "\n",
        "# Convert One-Hot Encoded array to DataFrame\n",
        "one_hot_df = pd.DataFrame(one_hot_encoded, columns=one_hot_encoder.get_feature_names_out())\n",
        "\n",
        "# Concatenate One-Hot Encoded columns with the original dataframe\n",
        "df_encoded = pd.concat([df_encoded, one_hot_df], axis=1)\n",
        "\n",
        "# Drop original categorical columns\n",
        "df_encoded.drop(columns=categorical_cols, inplace=True)\n",
        "\n",
        "# Identifying numerical columns (from df_encoded after encoding)\n",
        "numerical_cols = ['Age', 'Smoking', 'Alcohol_Consumption', 'BMI', 'Diabetes', 'Hypertension',\n",
        "                  'Cholesterol_Level', 'Resting_BP', 'Heart_Rate', 'Family_History',\n",
        "                  'Fasting_Blood_Sugar', 'Exercise_Induced_Angina', 'Max_Heart_Rate_Achieved']\n",
        "\n",
        "# Ensure numerical columns exist in df_encoded\n",
        "numerical_cols = [col for col in numerical_cols if col in df_encoded.columns]\n",
        "\n",
        "# Min-Max Scaling (Normalization: scales values between 0 and 1)\n",
        "min_max_scaler = MinMaxScaler()\n",
        "df_encoded[[col + '_MinMax' for col in numerical_cols]] = min_max_scaler.fit_transform(df_encoded[numerical_cols])\n",
        "\n",
        "# Standard Scaling (Z-score: mean = 0, std = 1)\n",
        "standard_scaler = StandardScaler()\n",
        "df_encoded[[col + '_ZScore' for col in numerical_cols]] = standard_scaler.fit_transform(df_encoded[numerical_cols])\n",
        "\n",
        "# Display first few rows after scaling\n",
        "print(df_encoded.head())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "HZfT61_2GCwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SKILL EXPERIMENT 5-6**"
      ],
      "metadata": {
        "id": "Y1Yc0x7kFRio"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create basic visualizations like histograms, bar charts, and scatter plots to explore relationships between features and identify patterns in the dataset.\n",
        "and\n",
        "Develop advanced visualizations such as heatmaps, box plots, and correlation matrices to gain deeper insights into the data and identify feature relationships."
      ],
      "metadata": {
        "id": "do-in9veTyDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(r\"/content/drive/My Drive/hi2.csv\")\n",
        "\n",
        "# Display the first 200 rows of the dataset\n",
        "df.head(200)\n",
        "\n",
        "# Plot 1: Histogram of Age with KDE\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['Age'], kde=True, bins=30)\n",
        "plt.title('Age Distribution')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Plot 2: Histogram of Heart Rate with KDE\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['Heart_Rate'], kde=True, bins=30)\n",
        "plt.title('Heart Rate Distribution')\n",
        "plt.xlabel('Heart Rate')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Plot 3: Histogram of Resting BP with KDE\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['Resting_BP'], kde=True, bins=30)\n",
        "plt.title('Resting BP Distribution')\n",
        "plt.xlabel('Resting BP')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Plot 4: Scatterplot of Resting_BP vs. Heart_Rate, colored by Heart_Attack_Risk\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=df, x=\"Resting_BP\", y=\"Heart_Rate\", hue=\"Heart_Attack_Risk\", palette='viridis')\n",
        "plt.title('Resting BP vs. Heart Rate')\n",
        "plt.xlabel('Resting BP')\n",
        "plt.ylabel('Heart Rate')\n",
        "plt.legend(title=\"Heart Attack Risk\")\n",
        "plt.show()\n",
        "\n",
        "# Plot 5: Scatterplot of Age vs. Heart_Rate, colored by Heart_Attack_Risk\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=df, x=\"Age\", y=\"Heart_Rate\", hue=\"Heart_Attack_Risk\", palette='coolwarm')\n",
        "plt.title('Age vs. Heart Rate')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Heart Rate')\n",
        "plt.legend(title=\"Heart Attack Risk\")\n",
        "plt.show()\n",
        "\n",
        "# Plot 6: Bar plot of Age vs. Heart Rate\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Age', y=\"Heart_Rate\", data=df)  # ci=None to avoid confidence intervals\n",
        "plt.title('Age vs. Heart Rate')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Heart Rate')\n",
        "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
        "plt.show()\n",
        "\n",
        "# Plot 7: Box plot of Heart Rate vs. Stress Level\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='Heart_Rate', y=\"Stress_Level\", data=df)\n",
        "plt.title('Heart Rate vs. Stress Level')\n",
        "plt.xlabel('Heart Rate')\n",
        "plt.ylabel('Stress Level')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.lineplot(data=df, x=\"Age\",y=\"Heart_Attack_Risk\", label=\"lineplot with marker\", color=\"blue\", marker=\"o\" )\n",
        "plt.title('Age vs.Heart_Attack_Risk')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Heart_Attack_Risk')\n",
        "plt.show()\n",
        "# Count the occurrences of each unique value in the 'Heart_Attack_Risk' column\n",
        "risk_counts = df['Heart_Attack_Risk'].value_counts()\n",
        "\n",
        "# Create a pie chart\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(risk_counts, labels=risk_counts.index, autopct='%1.1f%%', startangle=90, colors=['lightblue', 'salmon','black'])\n",
        "plt.title('Heart Attack Risk Distribution')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ePMLNmKdFQgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **USING Z-SCORE METHOD FOR REMOVING OUTLIERS**"
      ],
      "metadata": {
        "id": "oCDmgK7iIRLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/drive/MyDrive/hi.csv\"  # Update path if needed\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Calculate Z-scores for each numeric column\n",
        "for col in df.columns:\n",
        "    if df[col].dtype in ['int64', 'float64']:\n",
        "        z_scores = zscore(df[col])\n",
        "        print(f\"Z-scores for column '{col}':\")\n",
        "        print(z_scores)\n",
        "\n",
        "# --- Function to remove outliers using IQR ---\n",
        "def remove_outliers_iqr(df):\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype in ['int64', 'float64']:  # Only process numerical columns\n",
        "            Q1 = df[col].quantile(0.25)\n",
        "            Q3 = df[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "            # Filter out outliers\n",
        "            df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
        "    return df\n",
        "\n",
        "# Apply IQR to remove outliers\n",
        "df_no_outliers_iqr = remove_outliers_iqr(df)\n",
        "\n",
        "\n",
        "# --- Function to remove outliers using Z-score ---\n",
        "def remove_outliers_zscore(df, threshold=3):  # Default threshold set to 3\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype in ['int64', 'float64']:  # Only process numerical columns\n",
        "            z_scores = zscore(df[col])\n",
        "            df = df[abs(z_scores) <= threshold]\n",
        "    return df\n",
        "\n",
        "# Apply Z-score to remove outliers\n",
        "df_no_outliers_zscore = remove_outliers_zscore(df)\n",
        "\n",
        "\n",
        "# --- Print DataFrames for Comparison ---\n",
        "print(\"\\n\\n=== Original DataFrame ===\")\n",
        "print(df.head())\n",
        "print(f\"\\nOriginal Shape: {df.shape}\")\n",
        "\n",
        "print(\"\\n\\n=== DataFrame after removing outliers using IQR ===\")\n",
        "print(df_no_outliers_iqr.head())\n",
        "print(f\"\\nShape after IQR: {df_no_outliers_iqr.shape}\")\n",
        "\n",
        "print(\"\\n\\n=== DataFrame after removing outliers using Z-score ===\")\n",
        "print(df_no_outliers_zscore.head())\n",
        "print(f\"\\nShape after Z-score: {df_no_outliers_zscore.shape}\")\n",
        "\n",
        "\n",
        "# --- Save results to Google Drive ---\n",
        "# Correct Google Drive paths\n",
        "output_path_iqr = \"/content/drive/MyDrive/hi3.csv\"  # IQR cleaned data\n",
        "df_no_outliers_iqr.to_csv(output_path_iqr, index=False)\n",
        "\n",
        "output_path_zscore = \"/content/drive/MyDrive/hi4.csv\"  # Z-score cleaned data\n",
        "df_no_outliers_zscore.to_csv(output_path_zscore, index=False)\n",
        "\n",
        "print(\"\\n Cleaned datasets saved successfully to Google Drive!\")\n"
      ],
      "metadata": {
        "id": "PokMtTENIWfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SKILL EXPERIMENT 7-8**"
      ],
      "metadata": {
        "id": "JNgCbqB2IjI0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pearsons Correlation Test:"
      ],
      "metadata": {
        "id": "tjDG206XOq3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pearsonâ€™s Correlation Test measures the strength and direction of the linear relationship between two continuous variables. It returns a value (r) between -1 and 1, where 1 indicates a strong positive correlation, -1 indicates a strong negative correlation, and 0 means no correlation. This test is commonly used to understand how changes in one variable affect another."
      ],
      "metadata": {
        "id": "vF2QwLKOUYrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import pearsonr\n",
        "file_path = \"/content/archive.csv\"  # Update path if needed\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Define numerical features and target column\n",
        "target_column = 'Starting_Salary'  # Replace with actual numeric target if different\n",
        "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "# Check if the target column exists in your DataFrame\n",
        "if target_column not in df.columns:\n",
        "    raise KeyError(f\"The target column '{target_column}' is not found in the DataFrame.\")\n",
        "\n",
        "# Compute Pearson correlation for each feature with target\n",
        "correlation_results = {}\n",
        "\n",
        "print(\"\\n===== Pearson Correlation Test =====\")\n",
        "for feature in numerical_features:\n",
        "    if feature != target_column:\n",
        "        corr, _ = pearsonr(df[feature], df[target_column])\n",
        "        correlation_results[feature] = corr\n",
        "        print(f\"Feature: {feature}, Correlation: {corr}\")\n",
        "\n",
        "# Apply threshold-based filtering\n",
        "threshold = 0.2  # Change this as needed (absolute correlation threshold)\n",
        "filtered_features = {k: v for k, v in correlation_results.items() if abs(v) >= threshold}\n",
        "\n",
        "# Display filtered features\n",
        "print(\"\\n===== Features Above Correlation Threshold =====\")\n",
        "for feature, corr in filtered_features.items():\n",
        "    print(f\"Feature: {feature}, Correlation: {corr}\")\n",
        "\n",
        "# Create correlation matrix\n",
        "correlation_matrix = df[numerical_features].corr()\n",
        "\n",
        "# Display correlation matrix\n",
        "print(\"\\n===== Correlation Matrix =====\")\n",
        "print(correlation_matrix)\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Feature Correlation Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nPC9x3lRIl7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANOVA:"
      ],
      "metadata": {
        "id": "cabnEf8hOsRs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANOVA (Analysis of Variance) is used to compare the averages of three or more groups to see if they are different. It checks the variation within and between the groups. If the p-value is less than 0.05, it means at least one group is different from the others.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jrfImz_8VKZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import f_classif\n",
        "\n",
        "# Define numerical features and target variable\n",
        "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "target_column = 'Job_Offers'\n",
        "\n",
        "# Ensure the target column is numeric (ANOVA requires numeric target categories)\n",
        "df[target_column] = pd.to_numeric(df[target_column])\n",
        "\n",
        "# Perform ANOVA F-Test\n",
        "f_scores, p_values = f_classif(df[numerical_features], df[target_column])\n",
        "\n",
        "# Display feature scores\n",
        "print(\"\\n===== ANOVA Feature Selection Results =====\")\n",
        "for feature, score, p_value in zip(numerical_features, f_scores, p_values):\n",
        "    print(f\"Feature: {feature}, Score: {score:.2f}, p-value: {p_value:.4f}\")\n",
        "\n",
        "# Optional: Filter features with a p-value less than 0.05 (statistically significant)\n",
        "significant_features = [feature for feature, p in zip(numerical_features, p_values) if p < 0.05]\n",
        "\n",
        "print(\"\\nStatistically Significant Features (p < 0.05):\")\n",
        "print(significant_features)"
      ],
      "metadata": {
        "id": "DQBCzehsO02T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chi-Square:"
      ],
      "metadata": {
        "id": "qsxRo5xpO32w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chi-Square Test is used to check if there is a connection between two categories. It compares what we expect to happen with what actually happened. If the difference is big, it means the categories are related."
      ],
      "metadata": {
        "id": "en5SYWoLVxvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# Define categorical features and target variable\n",
        "categorical_features = ['Gender', 'Field_of_Study', 'Current_Job_Level', 'Entrepreneurship']\n",
        "target_column = 'Job_Offers'  # Ensure this is a categorical variable\n",
        "\n",
        "# Convert categorical features into numerical using one-hot encoding\n",
        "df_encoded = pd.get_dummies(df[categorical_features])\n",
        "\n",
        "# Perform Chi-Square test using SelectKBest\n",
        "k = 5  # Number of top features to select\n",
        "selector = SelectKBest(score_func=chi2, k='all')  # Initially select all\n",
        "X_new = selector.fit_transform(df_encoded, df[target_column])\n",
        "\n",
        "# Get feature scores\n",
        "feature_scores = selector.scores_\n",
        "threshold = np.percentile(feature_scores, 50)  # Select features above median score\n",
        "\n",
        "# Get top-K features\n",
        "top_k_selector = SelectKBest(score_func=chi2, k=k)\n",
        "X_k_new = top_k_selector.fit_transform(df_encoded, df[target_column])\n",
        "selected_k_features = np.array(df_encoded.columns)[top_k_selector.get_support()]\n",
        "top_k_scores = top_k_selector.scores_[top_k_selector.get_support()]\n",
        "\n",
        "# Display results for threshold-based selection\n",
        "print(\"\\n===== Chi-Square Features (Above Threshold) =====\")\n",
        "for feature, score in zip(df_encoded.columns, feature_scores):\n",
        "    if score >= threshold:\n",
        "        print(f\"Feature: {feature}, Score: {score}\")\n",
        "\n",
        "# Display results for top-K selection\n",
        "print(\"\\n===== Top-K Chi-Square Features =====\")\n",
        "for feature, score in zip(selected_k_features, top_k_scores):\n",
        "    print(f\"Feature: {feature}, Score: {score}\")"
      ],
      "metadata": {
        "id": "SKpApyshO4aO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Information Gain:"
      ],
      "metadata": {
        "id": "bhub7nPFO9GV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Information Gain is a measure used in decision trees to determine how well a feature separates the data. It calculates the reduction in uncertainty (entropy) after splitting the data on a feature. A higher information gain means the feature is more useful for making decisions."
      ],
      "metadata": {
        "id": "Pe4CcXSjWGl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
        "\n",
        "# Define target variable\n",
        "target_column = 'Career_Satisfaction'  # Replace with actual categorical target\n",
        "\n",
        "# Encode categorical target\n",
        "df[target_column] = df[target_column].astype('category').cat.codes\n",
        "\n",
        "# Select numerical features\n",
        "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "# Compute mutual information (information gain)\n",
        "mi_scores = mutual_info_classif(df[numerical_features], df[target_column])\n",
        "\n",
        "# Apply threshold-based filtering\n",
        "threshold = np.percentile(mi_scores, 50)  # Select features above the 50th percentile\n",
        "\n",
        "# Get top-K features\n",
        "k = 5  # Number of top features to select\n",
        "top_k_selector = SelectKBest(score_func=mutual_info_classif, k=k)\n",
        "X_k_new = top_k_selector.fit_transform(df[numerical_features], df[target_column])\n",
        "selected_k_features = np.array(numerical_features)[top_k_selector.get_support()]\n",
        "top_k_scores = top_k_selector.scores_[top_k_selector.get_support()]\n",
        "\n",
        "# Display results for threshold-based selection\n",
        "print(\"\\n===== Information Gain (Above Threshold) =====\")\n",
        "for feature, score in zip(numerical_features, mi_scores):\n",
        "    if score >= threshold:\n",
        "        print(f\"Feature: {feature}, Information Gain: {score}\")\n",
        "\n",
        "# Display results for top-K selection\n",
        "print(\"\\n===== Top-K Information Gain Features =====\")\n",
        "for feature, score in zip(selected_k_features, top_k_scores):\n",
        "    print(f\"Feature: {feature}, Information Gain: {score}\")"
      ],
      "metadata": {
        "id": "ofy26emmO-dM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SKILL EXPERIMENT 9-10**"
      ],
      "metadata": {
        "id": "Rn_4AVi0PGfX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward Selection:"
      ],
      "metadata": {
        "id": "A1hqHCM2PNNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward selection starts with no features and adds one feature at a time based on which improves the model the most, stopping when no significant improvement is observed."
      ],
      "metadata": {
        "id": "LsuyET-OXcoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load data\n",
        "file_path = \"/content/archive.csv\"  # Update path if needed\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Define features and target\n",
        "X = df.drop('Job_Offers', axis=1)\n",
        "y = df['Job_Offers']\n",
        "\n",
        "# Encode categorical columns\n",
        "for col in X.select_dtypes(include='object').columns:\n",
        "    X[col] = LabelEncoder().fit_transform(X[col])\n",
        "\n",
        "# Forward Selection\n",
        "selected_features = []\n",
        "remaining_features = list(X.columns)\n",
        "\n",
        "for _ in range(len(remaining_features)):\n",
        "    best_score = float('-inf')\n",
        "    best_feature = None\n",
        "\n",
        "    for feature in remaining_features:\n",
        "        trial_features = selected_features + [feature]\n",
        "        X_trial = sm.add_constant(X[trial_features])\n",
        "        model = sm.OLS(y, X_trial).fit()\n",
        "\n",
        "        if model.rsquared > best_score:\n",
        "            best_score = model.rsquared\n",
        "            best_feature = feature\n",
        "\n",
        "    if best_feature is not None:\n",
        "        selected_features.append(best_feature)\n",
        "        remaining_features.remove(best_feature)\n",
        "\n",
        "print(\"Selected Features (Forward Selection):\", selected_features)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Egb2ftf1PKW0",
        "outputId": "081f53c0-30c8-4a42-baf3-2deaf48b8ad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Features (Forward Selection): ['Starting_Salary', 'Internships_Completed', 'Projects_Completed', 'Age', 'SAT_Score', 'Networking_Score', 'Years_to_Promotion', 'High_School_GPA', 'University_GPA', 'Gender', 'University_Ranking', 'Entrepreneurship', 'Work_Life_Balance', 'Current_Job_Level', 'Student_ID', 'Soft_Skills_Score', 'Certifications', 'Field_of_Study', 'Career_Satisfaction']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "backward Elimination."
      ],
      "metadata": {
        "id": "8Zv2fAE5Qutb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backward elimination starts with all features and removes the least significant one at each step until removing features no longer improves the model."
      ],
      "metadata": {
        "id": "jPrI9MFmXgGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Start with all features\n",
        "features = list(X.columns)\n",
        "X_with_const = sm.add_constant(X[features])\n",
        "\n",
        "while len(features) > 0:\n",
        "    model = sm.OLS(y, X_with_const).fit()\n",
        "    p_values = model.pvalues[1:]\n",
        "\n",
        "    worst_p = p_values.idxmax()\n",
        "    if p_values[worst_p] > 0.05:\n",
        "        features.remove(worst_p)\n",
        "        X_with_const = sm.add_constant(X[features])\n",
        "    else:\n",
        "        break\n",
        "\n",
        "print(\"Selected Features (Backward Elimination):\", features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tl8yT0uXQvKS",
        "outputId": "f619ada7-fe5b-4cad-d757-7065ce319381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Features (Backward Elimination): ['Age', 'Internships_Completed', 'Projects_Completed', 'Starting_Salary']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recursive Feature Elimination (RFE):"
      ],
      "metadata": {
        "id": "-acKgcZ_Q0nc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RFE selects the best features by recursively removing the least important features and building the model repeatedly until the desired number of features is reached."
      ],
      "metadata": {
        "id": "RFU2yy3dXjPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "model = RandomForestRegressor()\n",
        "rfe = RFE(model, n_features_to_select=5)\n",
        "fit = rfe.fit(X, y)\n",
        "\n",
        "selected_features = X.columns[fit.support_].tolist()\n",
        "print(\"Selected Features (RFE):\", selected_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eWaihuqQ09r",
        "outputId": "14481d85-41b3-4971-910d-14433527e68f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Features (RFE): ['Student_ID', 'SAT_Score', 'University_Ranking', 'University_GPA', 'Starting_Salary']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross Validation (CV):"
      ],
      "metadata": {
        "id": "E7YEliB9Q9b-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-validation splits the data into multiple parts, trains the model on one part, and tests it on the other parts to ensure the model performs well on unseen data."
      ],
      "metadata": {
        "id": "aE9N9ESOXlT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "model = RandomForestRegressor()\n",
        "cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
        "\n",
        "print(\"Cross-Validation Scores:\", cv_scores)\n",
        "print(\"Average CV Score:\", cv_scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwxTPIRbQ-Bs",
        "outputId": "c6fe017c-6b08-473d-e021-a7e1675eeb9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation Scores: [-0.01634611 -0.04564856 -0.02752528 -0.01253433 -0.02856999]\n",
            "Average CV Score: -0.0261248537047885\n"
          ]
        }
      ]
    }
  ]
}